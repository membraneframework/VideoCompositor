"use strict";(self.webpackChunkcompositor_live=self.webpackChunkcompositor_live||[]).push([[1532],{64519:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>a,frontMatter:()=>s,metadata:()=>d,toc:()=>p});var n=i(74848),o=i(28453);const s={title:"RTP",description:"RTP Output"},r="RTP",d={id:"typescript/inputs/rtp",title:"RTP",description:"RTP Output",source:"@site/pages/typescript/inputs/rtp.md",sourceDirName:"typescript/inputs",slug:"/typescript/inputs/rtp",permalink:"/docs/typescript/inputs/rtp",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"RTP",description:"RTP Output"},sidebar:"sidebar",previous:{title:"MP4",permalink:"/docs/typescript/outputs/mp4"},next:{title:"MP4",permalink:"/docs/typescript/inputs/mp4"}},c={},p=[{value:"<code>Inputs.RegisterRtpInput</code>",id:"inputsregisterrtpinput",level:3},{value:"Properties",id:"properties",level:4},{value:"<code>Inputs.InputRtpVideoOptions</code>",id:"inputsinputrtpvideooptions",level:3},{value:"<code>Inputs.InputRtpAudioOptions</code>",id:"inputsinputrtpaudiooptions",level:3},{value:"Properties (<code>decoder: &quot;opus&quot;</code>)",id:"properties-decoder-opus",level:4},{value:"Properties (<code>decoder: &quot;aac&quot;</code>)",id:"properties-decoder-aac",level:4}];function l(e){const t={a:"a",code:"code",h1:"h1",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"rtp",children:"RTP"}),"\n",(0,n.jsx)(t.p,{children:"An input type that allows streaming video and audio to the compositor over RTP."}),"\n",(0,n.jsx)(t.h3,{id:"inputsregisterrtpinput",children:(0,n.jsx)(t.code,{children:"Inputs.RegisterRtpInput"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-typescript",children:'import { Inputs } from "live-compositor"\n\ntype RegisterRtpInput = {\n  port: string | u16;\n  transportProtocol?: "udp" | "tcp_server";\n  video?: Inputs.InputRtpVideoOptions;\n  audio?: Inputs.InputRtpAudioOptions;\n  required?: bool;\n  offsetMs?: number;\n}\n'})}),"\n",(0,n.jsxs)(t.p,{children:["Parameters for an input stream from RTP source.\nAt least one of ",(0,n.jsx)(t.code,{children:"video"})," and ",(0,n.jsx)(t.code,{children:"audio"})," has to be defined."]}),"\n",(0,n.jsx)(t.h4,{id:"properties",children:"Properties"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"port"})," - UDP port or port range on which the compositor should listen for the stream."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"transportProtocol"})," - Transport protocol.","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:'"udp"'})," - UDP protocol."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:'"tcp_server"'})," - TCP protocol where LiveCompositor is the server side of the connection."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"video"})," - Parameters of a video source included in the RTP stream."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"audio"})," - Parameters of an audio source included in the RTP stream."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"required"})," - (",(0,n.jsxs)(t.strong,{children:["default=",(0,n.jsx)(t.code,{children:"false"})]}),") If input is required and the stream is not delivered\non time, then LiveCompositor will delay producing output frames."]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"offsetMs"})," - Offset in milliseconds relative to the pipeline start (start request). If the offset is\nnot defined then the stream will be synchronized based on the delivery time of the initial\nframes."]}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"inputsinputrtpvideooptions",children:(0,n.jsx)(t.code,{children:"Inputs.InputRtpVideoOptions"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-typescript",children:'type InputRtpVideoOptions = {\n  decoder: "ffmpeg_h264";\n}\n'})}),"\n",(0,n.jsx)(t.h3,{id:"inputsinputrtpaudiooptions",children:(0,n.jsx)(t.code,{children:"Inputs.InputRtpAudioOptions"})}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{className:"language-typescript",children:'type InputRtpAudioOptions = \n  | {\n      decoder: "opus";\n      forwardErrorCorrection?: bool;\n    }\n  | {\n      decoder: "aac";\n      audioSpecificConfig: string;\n      rtpMode?: "low_bitrate" | "high_bitrate";\n    }\n'})}),"\n",(0,n.jsxs)(t.h4,{id:"properties-decoder-opus",children:["Properties (",(0,n.jsx)(t.code,{children:'decoder: "opus"'}),")"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.code,{children:"forwardErrorCorrection"})," - (",(0,n.jsxs)(t.strong,{children:["default=",(0,n.jsx)(t.code,{children:"false"})]}),") Specifies whether the stream uses forward error correction.\nIt's specific for Opus codec.\nFor more information, check out ",(0,n.jsx)(t.a,{href:"https://datatracker.ietf.org/doc/html/rfc6716#section-2.1.7",children:"RFC"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(t.h4,{id:"properties-decoder-aac",children:["Properties (",(0,n.jsx)(t.code,{children:'decoder: "aac"'}),")"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.code,{children:"audioSpecificConfig"})," - AudioSpecificConfig as described in MPEG-4 part 3, section 1.6.2.1\nThe config should be encoded as described in ",(0,n.jsx)(t.a,{href:"https://datatracker.ietf.org/doc/html/rfc3640#section-4.1",children:"RFC 3640"}),"."]}),"\n",(0,n.jsxs)(t.p,{children:["The simplest way to obtain this value when using ffmpeg to stream to the compositor is\nto pass the additional ",(0,n.jsx)(t.code,{children:"-sdp_file FILENAME"})," option to ffmpeg. This will cause it to\nwrite out an sdp file, which will contain this field. Programs which have the ability\nto stream AAC to the compositor should provide this information."]}),"\n",(0,n.jsx)(t.p,{children:"In MP4 files, the ASC is embedded inside the esds box (note that it is not the whole\nbox, only a part of it). This also applies to fragmented MP4s downloaded over HLS, if\nthe playlist uses MP4s instead of MPEG Transport Streams"}),"\n",(0,n.jsxs)(t.p,{children:["In FLV files and the RTMP protocol, the ASC can be found in the ",(0,n.jsx)(t.code,{children:"AACAUDIODATA"})," tag."]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.code,{children:"rtpMode"})," - (",(0,n.jsxs)(t.strong,{children:["default=",(0,n.jsx)(t.code,{children:'"high_bitrate"'})]}),")\nSpecifies the ",(0,n.jsx)(t.a,{href:"https://datatracker.ietf.org/doc/html/rfc3640#section-3.3.1",children:"RFC 3640 mode"}),"\nthat should be used when depacketizing this stream."]}),"\n"]}),"\n"]})]})}function a(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}},28453:(e,t,i)=>{i.d(t,{R:()=>r,x:()=>d});var n=i(96540);const o={},s=n.createContext(o);function r(e){const t=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),n.createElement(s.Provider,{value:t},e.children)}}}]);