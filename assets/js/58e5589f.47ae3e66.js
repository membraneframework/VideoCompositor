"use strict";(self.webpackChunkcompositor_live=self.webpackChunkcompositor_live||[]).push([[1872],{6914:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>d,toc:()=>c});var s=t(5893),i=t(1151);const r={},o=void 0,d={id:"api/generated/output-OutputStream",title:"output-OutputStream",description:"OutputStream",source:"@site/pages/api/generated/output-OutputStream.md",sourceDirName:"api/generated",slug:"/api/generated/output-OutputStream",permalink:"/docs/api/generated/output-OutputStream",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{}},l={},c=[{value:"OutputStream",id:"outputstream",level:2},{value:"Properties",id:"properties",level:4},{value:"OutputVideoOptions",id:"outputvideooptions",level:2},{value:"Properties",id:"properties-1",level:4},{value:"OutputAudioOptions",id:"outputaudiooptions",level:2},{value:"Properties",id:"properties-2",level:4},{value:"OutputEndCondition",id:"outputendcondition",level:2},{value:"Properties",id:"properties-3",level:4},{value:"InputAudio",id:"inputaudio",level:2},{value:"Properties",id:"properties-4",level:4}];function u(e){const n={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"outputstream",children:"OutputStream"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'type OutputStream = {\n  port: string | u16;\n  ip?: string;\n  transport_protocol?: "udp" | "tcp_server";\n  video?: OutputVideoOptions;\n  audio?: OutputAudioOptions;\n}\n'})}),"\n",(0,s.jsx)(n.h4,{id:"properties",children:"Properties"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"port"})," - Depends on the value of the ",(0,s.jsx)(n.code,{children:"transport_protocol"})," field:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"udp"})," - An UDP port number that RTP packets will be sent to."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"tcp_server"})," - A local TCP port number or a port range that LiveCompositor will listen for incoming connections."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ip"})," - Only valid if ",(0,s.jsx)(n.code,{children:'transport_protocol="udp"'}),". IP address where RTP packets should be sent to."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"transport_protocol"})," - (",(0,s.jsxs)(n.strong,{children:["default=",(0,s.jsx)(n.code,{children:'"udp"'})]}),") Transport layer protocol that will be used to send RTP packets.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"udp"'})," - UDP protocol."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"tcp_server"'})," - TCP protocol where LiveCompositor is the server side of the connection."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"outputvideooptions",children:"OutputVideoOptions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'type OutputVideoOptions = {\n  resolution: { width: u32; height: u32 };\n  encoder_preset: \n    | "ultrafast"\n    | "superfast"\n    | "veryfast"\n    | "faster"\n    | "fast"\n    | "medium"\n    | "slow"\n    | "slower"\n    | "veryslow"\n    | "placebo";\n  ffmpeg_options?: Map<string, string>;\n  initial: Component;\n  send_eos_when?: OutputEndCondition;\n}\n'})}),"\n",(0,s.jsx)(n.h4,{id:"properties-1",children:"Properties"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"resolution"})," - Output resolution in pixels."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"encoder_preset"})," - (",(0,s.jsxs)(n.strong,{children:["default=",(0,s.jsx)(n.code,{children:'"fast"'})]}),") Preset for an encoder. See ",(0,s.jsx)(n.code,{children:"FFmpeg"})," ",(0,s.jsx)(n.a,{href:"https://trac.ffmpeg.org/wiki/Encode/H.264#Preset",children:"docs"})," to learn more."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ffmpeg_options"})," - Raw FFmpeg encoder options. See ",(0,s.jsx)(n.a,{href:"https://ffmpeg.org/ffmpeg-codecs.html",children:"docs"})," for more."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"initial"})," - Root of a component tree/scene that should be rendered for the output. Use ",(0,s.jsxs)(n.a,{href:"/docs/api/routes#update-output",children:[(0,s.jsx)(n.code,{children:"update_output"})," request"]})," to update this value after registration. ",(0,s.jsx)(n.a,{href:"/docs/concept/component",children:"Learn more"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"send_eos_when"})," - Defines when output stream should end if some of the input streams are finished. If output includes both audio and video streams, then EOS needs to be sent on both."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"outputaudiooptions",children:"OutputAudioOptions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'type OutputAudioOptions = {\n  initial: { inputs: InputAudio[] };\n  channels: "mono" | "stereo";\n  forward_error_correction?: bool;\n  encoder_preset?: "quality" | "voip" | "lowest_latency";\n  mixing_strategy?: "sum_clip" | "sum_scale";\n  send_eos_when?: OutputEndCondition;\n}\n'})}),"\n",(0,s.jsx)(n.h4,{id:"properties-2",children:"Properties"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"initial"})," - Initial audio for output."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"channels"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"mono"'})," - Mono audio (single channel)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"stereo"'})," - Stereo audio (two channels)."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"forward_error_correction"})," - (",(0,s.jsxs)(n.strong,{children:["default=",(0,s.jsx)(n.code,{children:"false"})]}),") Specifies whether the stream use forward error correction. It's specific for Opus codec. For more information, check out ",(0,s.jsx)(n.a,{href:"https://datatracker.ietf.org/doc/html/rfc6716#section-2.1.7",children:"RFC"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"encoder_preset"})," - (",(0,s.jsx)(n.strong,{children:'default="voip"'}),") Specifies preset for audio output encoder.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"quality"'})," - Best for broadcast/high-fidelity application where the decoded audio should be as close as possible to the input."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"voip"'})," - Best for most VoIP/videoconference applications where listening quality and intelligibility matter most."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"lowest_latency"'})," - Only use when lowest-achievable latency is what matters most."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"mixing_strategy"}),' - (**default="sum_clip") Specifies how audio should be mixed.',"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"sum_clip"'})," - Firstly, input samples are summed. If the result is outside the i16 PCM range, it gets clipped."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:'"sum_scale"'})," - Firstly, input samples are summed. If the result is outside the i16 PCM range, nearby summed samples are scaled down by factor, such that the summed wave is in the i16 PCM range."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"send_eos_when"})," - Condition for termination of output stream based on the input streams states."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"outputendcondition",children:"OutputEndCondition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"type OutputEndCondition = {\n  any_of?: string[];\n  all_of?: string[];\n  any_input?: bool;\n  all_inputs?: bool;\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:"This type defines when end of an input stream should trigger end of the output stream. Only one of those fields can be set at the time."}),"\n",(0,s.jsx)(n.p,{children:"Unless specified otherwise the input stream is considered finished/ended when:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"TCP connection was dropped/closed."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["RTCP Goodbye packet (",(0,s.jsx)(n.code,{children:"BYE"}),") was received."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Mp4 track has ended."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Input was unregistered already (or never registered)."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"properties-3",children:"Properties"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"any_of"})," - Terminate output stream if any of the input streams from the list are finished."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"all_of"})," - Terminate output stream if all the input streams from the list are finished."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"any_input"})," - Terminate output stream if any of the input streams ends. This includes streams added after the output was registered. In particular, output stream will ",(0,s.jsx)(n.strong,{children:"not be"})," terminated if no inputs were ever connected."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"all_inputs"})," - Terminate output stream if all the input streams finish. In particular, output stream will ",(0,s.jsx)(n.strong,{children:"be"})," terminated if no inputs were ever connected."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"inputaudio",children:"InputAudio"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"type InputAudio = {\n  input_id: string;\n  volume?: f32;\n}\n"})}),"\n",(0,s.jsx)(n.h4,{id:"properties-4",children:"Properties"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"volume"})," - (",(0,s.jsxs)(n.strong,{children:["default=",(0,s.jsx)(n.code,{children:"1.0"})]}),") float in ",(0,s.jsx)(n.code,{children:"[0, 1]"})," range representing input volume"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>d,a:()=>o});var s=t(7294);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);