"use strict";(self.webpackChunkcompositor_live=self.webpackChunkcompositor_live||[]).push([[692],{4011:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>d,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var t=r(5893),i=r(1151);const s={},d=void 0,o={id:"api/generated/renderer-RtpInputStream",title:"renderer-RtpInputStream",description:"RtpInputStream",source:"@site/pages/api/generated/renderer-RtpInputStream.md",sourceDirName:"api/generated",slug:"/api/generated/renderer-RtpInputStream",permalink:"/docs/api/generated/renderer-RtpInputStream",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{}},c={},l=[{value:"RtpInputStream",id:"rtpinputstream",level:2},{value:"Properties",id:"properties",level:4},{value:"Port",id:"port",level:2},{value:"Video",id:"video",level:2},{value:"Properties",id:"properties-1",level:4},{value:"Audio",id:"audio",level:2},{value:"Properties",id:"properties-2",level:4}];function a(e){const n={a:"a",code:"code",h2:"h2",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"rtpinputstream",children:"RtpInputStream"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:"type RtpInputStream = {\n  input_id: string;\n  port: Port;\n  video?: Video;\n  audio?: Audio;\n  required?: bool;\n  offset_ms?: f64;\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Parameters for an input stream from RTP source. At least one of ",(0,t.jsx)(n.code,{children:"video"})," and ",(0,t.jsx)(n.code,{children:"audio"})," has to be defined."]}),"\n",(0,t.jsx)(n.h4,{id:"properties",children:"Properties"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"input_id"})," - An identifier for the input stream."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"port"})," - UDP port or port range on which the compositor should listen for the stream."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"video"})," - Parameters of a video source included in the RTP stream."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"audio"})," - Parameters of an audio source included in the RTP stream."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"required"})," - (",(0,t.jsxs)(n.strong,{children:["default=",(0,t.jsx)(n.code,{children:"false"})]}),") If input is required and the stream is not delivered on time, then LiveCompositor will delay producing output frames."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"offset_ms"})," - Offset in milliseconds relative to the pipeline start (start request). If offset is not defined then stream is synchronized based on the first frames delivery time."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"port",children:"Port"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:"type Port = string | u16\n"})}),"\n",(0,t.jsx)(n.h2,{id:"video",children:"Video"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'type Video = {\n  codec?: "h264";\n  rtp_payload_type?: u8;\n}\n'})}),"\n",(0,t.jsx)(n.h4,{id:"properties-1",children:"Properties"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"codec"})," - (",(0,t.jsxs)(n.strong,{children:["default=",(0,t.jsx)(n.code,{children:'"h264"'})]}),") Video codec.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"h264"'})," - H264 video."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rtp_payload_type"})," - (",(0,t.jsxs)(n.strong,{children:["default=",(0,t.jsx)(n.code,{children:"96"})]}),") Value of payload type field in received RTP packets.\nPackets with different payload type won't be treated as video and included in composing. Values should be in [0, 64] or [96, 255]. Values in range [65, 95] can't be used. For more information, see ",(0,t.jsx)(n.a,{href:"https://datatracker.ietf.org/doc/html/rfc5761#section-4",children:"RFC"})," Packets with different payload type won't be treated as video and included in composing."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"audio",children:"Audio"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-typescript",children:'type Audio = {\n  codec?: "opus";\n  sample_rate: u32;\n  channels: "mono" | "stereo";\n  rtp_payload_type?: u8;\n  forward_error_correction?: bool;\n}\n'})}),"\n",(0,t.jsx)(n.h4,{id:"properties-2",children:"Properties"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"codec"})," - (",(0,t.jsxs)(n.strong,{children:["default=",(0,t.jsx)(n.code,{children:'"opus"'})]}),") Audio codec.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"opus"'})," - Opus audio."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"sample_rate"})," - Sample rate. If the specified sample rate doesn't match real sample rate, audio won't be mixed properly."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"channels"})," - Audio channels.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"mono"'})," - Mono audio (single channel)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:'"stereo"'})," - Stereo audio (two channels)."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rtp_payload_type"})," - (",(0,t.jsxs)(n.strong,{children:["default=",(0,t.jsx)(n.code,{children:"97"})]}),") Value of payload type field in received RTP packets.\nPackets with different payload type won't be treated as audio and included in mixing. Values should be in range [0, 64] or [96, 255]. Values in range [65, 95] can't be used. For more information, check out ",(0,t.jsx)(n.a,{href:"https://datatracker.ietf.org/doc/html/rfc5761#section-4",children:"RFC"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"forward_error_correction"})," - (",(0,t.jsxs)(n.strong,{children:["default=",(0,t.jsx)(n.code,{children:"false"})]}),") Specifies whether the stream uses forward error correction. It's specific for Opus codec. For more information, check out ",(0,t.jsx)(n.a,{href:"https://datatracker.ietf.org/doc/html/rfc6716#section-2.1.7",children:"RFC"}),"."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}},1151:(e,n,r)=>{r.d(n,{Z:()=>o,a:()=>d});var t=r(7294);const i={},s=t.createContext(i);function d(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:d(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);